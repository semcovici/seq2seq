{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EP3**\n",
    "*Nome (matrícula):* ___________________________________\n",
    "\n",
    "\n",
    "## **Introdução**\n",
    "Dando continuidade à atividade proposta na Quick Task 3, neste notebook implementaremos um modelo de tradução sequência para sequência (Seq2Seq) utilizando Redes Neurais Recorrentes (RNNs) para converter datas no formato legível por humanos (como \"12 de outubro de 2024\") para um formato legível por máquinas (como \"12/10/2024\").\n",
    "\n",
    "### **Contexto**\n",
    "\n",
    "Modelos Seq2Seq são amplamente utilizados em tarefas que envolvem a transformação de uma sequência de entrada em outra sequência de saída, como tradução automática de idiomas, sumarização de texto e, neste caso, a transformação de formatos de data. Esses modelos geralmente consistem em dois componentes principais:\n",
    "\n",
    "1. **Encoder**: Lê e compreende a sequência de entrada (neste caso, a data no formato legível por humanos).\n",
    "2. **Decoder**: Gera a sequência de saída correspondente (a data no formato legível por máquinas).\n",
    "\n",
    "### **Objetivo**\n",
    "\n",
    " O objetivo desta tarefa é aplicar os conceitos discutidos nos artigos \"Sequence to Sequence Learning with Neural Networks\" e \"Neural Machine Translation by Jointly Learning to Align and Translate\", utilizando-os em um problema de tradução de formatos de datas. Este exercício servirá como um exemplo prático do poder dos modelos Seq2Seq e do uso de RNNs em tarefas de tradução.\n",
    "\n",
    " Você deve rodar o código e tentar entender o que está acontecendo nas primeiras três seções. À medida que você se familiariza com a implementação, sinta-se à vontade para continuar a exploração em algum caminho que você achar interessante. Isso pode incluir experimentar diferentes configurações de modelo, aplicar novos datasets ou investigar alternativas aos mecanismos de atenção. O objetivo é que você desenvolva uma compreensão mais profunda dos modelos Seq2Seq e das suas aplicações práticas.\n",
    "\n",
    "Este notebook possui as seguintes seções:\n",
    "\n",
    "1. **Carregando os dados:** Iremos carregar um dataset pronto e em seguida converteremos as datas no formato de string para um formato numérico para que o modelo possa entender.\n",
    "2. **Construção do modelo sem atenção:** Implementaremos o modelo Seq2Seq com base em GRU, uma variante das RNNs.\n",
    "3. **Adicionando atenção:** Implementaremos uma versão do Decoder com atenção.\n",
    "4. **Exercícios de Exploração Sugeridos:** Sugestões de explorações para que você possa explorar um pouco mais a tarefa.\n",
    "\n",
    "\n",
    "\n",
    "### **Referências**\n",
    "\n",
    "Este EP foi baseado em dois notebooks, do primeiro retirou a ideia do dataset [1] e o código PyTorch foi adaptado do segundo [2]. Revisão no texto e alguns comentários nos códigos foram gerados com ChatGPT.\n",
    "\n",
    "[1] “Coursera-Deep-Learning-Specialization/C5 - Sequence Models/Week 3/Machine Translation/Neural Machine Translation with Attention - v4.Ipynb at Master · Amanchadha/Coursera-Deep-Learning-Specialization”. GitHub, https://github.com/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Machine%20Translation/Neural%20machine%20translation%20with%20attention%20-%20v4.ipynb. \n",
    "\n",
    "[2] NLP From Scratch: Translation with a Sequence to Sequence Network and Attention — PyTorch Tutorials 2.4.0+cu121 documentation. Por Sean Robertson. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device={device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 1. **Carregando os dados** \n",
    "\n",
    "Iremos carregar um dataset pronto e em seguida converteremos as datas no formato de string para um formato numérico que o modelo possa entender.\n",
    "\n",
    "Vamos treinar o modelo em um dataset de 10.000 datas legíveis por humanos e suas equivalentes, padronizadas, legíveis por máquinas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 10000 samples from datahumana-datamaquina.txt\n",
      "\n",
      "['10 10 08', '10/10/2008']\n",
      "['22 setembro 2018', '22/09/2018']\n",
      "['quarta-feira 1 de junho de 1994', '01/06/1994']\n",
      "['5 de outubro de 2000', '05/10/2000']\n",
      "['22 agosto 2018', '22/08/2018']\n",
      "['quarta-feira 13 de abril de 1994', '13/04/1994']\n",
      "['06.07.79', '06/07/1979']\n",
      "['quarta-feira 8 de novembro de 2023', '08/11/2023']\n",
      "['14 11 15', '14/11/2015']\n",
      "['30 de julho de 2019', '30/07/2019']\n",
      "['21.03.13', '21/03/2013']\n",
      "['18 ago. 2001', '18/08/2001']\n",
      "['13 set. 1997', '13/09/1997']\n",
      "['22 mar. 2020', '22/03/2020']\n",
      "['15 02 16', '15/02/2016']\n"
     ]
    }
   ],
   "source": [
    "def readDataset():\n",
    "  file = 'datahumana-datamaquina.txt'\n",
    "  # file = '' # Arquivo a ser lido na formatação entrada<tab>saida\n",
    "  lines = open(file, encoding='utf-8').read().strip().split('\\n')\n",
    "  pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "  print(f'Read {len(pairs)} samples from {file}')\n",
    "  print()\n",
    "  print(\"\\n\".join(map(str, random.sample(pairs, 15))))\n",
    "  return pairs\n",
    "\n",
    "pairs = readDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É necessário codificar as entradas e saídas em representações que o modelo consiga entender. Neste caso, representaremos as 'palavras' como inteiros atribuídos de forma sequencial.\n",
    "\n",
    "\n",
    "Além disso, é necessário definir os tokens especiais para o início e fim de uma sequência.\n",
    "- SOS_token (Start of Sequence - Início da Sequência) é usado para inicializar a entrada do decodificador ao gerar frases.\n",
    "- EOS_token (End of Sequence - Fim da Sequência) indica o fim da frase, para que o decodificador saiba quando parar de gerar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "def prepareData(pairs):\n",
    "  input_dict  = {\"SOS\": SOS_token, \"EOS\": EOS_token}\n",
    "  output_dict = {\"SOS\": SOS_token, \"EOS\": EOS_token}\n",
    "\n",
    "  input_dict_reverse  = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "  output_dict_reverse = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "\n",
    "  for pair in pairs:\n",
    "    ipt, tgt = pair\n",
    "    \n",
    "    mask = '([^a-zA-Z0-9])'\n",
    "    result = [token for token in re.split(mask, ipt) if token.strip()]\n",
    "    for word_input in result:\n",
    "      if word_input not in input_dict:\n",
    "        next_int = max(input_dict.values()) + 1\n",
    "        input_dict[word_input] = next_int\n",
    "        input_dict_reverse[next_int] = word_input\n",
    "\n",
    "    for word_output in re.split(mask, tgt):\n",
    "      if word_output not in output_dict:\n",
    "        next_int = max(output_dict.values()) + 1\n",
    "        output_dict[word_output] = next_int\n",
    "        output_dict_reverse[next_int] = word_output\n",
    "\n",
    "  print(f'Number of input words: {len(input_dict.keys())}')\n",
    "  print(f'Number of output words: {len(output_dict.keys())}')\n",
    "\n",
    "  return input_dict, output_dict, input_dict_reverse, output_dict_reverse\n",
    "\n",
    "input_dict, output_dict, input_dict_reverse, output_dict_reverse = prepareData(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(language_dict, sentence, verbose=False):\n",
    "  '''\n",
    "    Função utilitária para converter uma sentença em índices\n",
    "  '''\n",
    "  encoded = [language_dict[word] for word in re.split('([^a-zA-Z0-9])', sentence) if word.strip()]\n",
    "  if verbose:\n",
    "    print(f\"'{sentence}' => {encoded}\")\n",
    "  return encoded\n",
    "\n",
    "indexesFromSentence(input_dict, 'segunda-feira 18 de outubro de 1999', verbose=True);\n",
    "indexesFromSentence(output_dict, '30/09/2022', verbose=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções utilitárias\n",
    "def tensorFromSentence(language_dict, sentence):\n",
    "  indexes = indexesFromSentence(language_dict, sentence)\n",
    "  indexes.append(EOS_token)\n",
    "  return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_dict, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_dict, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "tensorsFromPair(['segunda-feira 18 de outubro de 1999', '18/10/1999'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando data loader para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size, pairs, input_dict, output_dict):\n",
    "  n = len(pairs)\n",
    "  input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "  target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "  for idx, (inp, tgt) in enumerate(pairs):\n",
    "    inp_ids = indexesFromSentence(input_dict, inp)\n",
    "    tgt_ids = indexesFromSentence(output_dict, tgt)\n",
    "    inp_ids.append(EOS_token)\n",
    "    tgt_ids.append(EOS_token)\n",
    "    input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "    target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "  train_data = TensorDataset(torch.LongTensor(input_ids).to(device), torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "  train_sampler = RandomSampler(train_data)\n",
    "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "  return train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 2. **Construção do modelo sem atenção:**\n",
    "\n",
    "Dando continuidade, vamos implementar o modelo Seq2Seq utilizando a célula GRU.\n",
    "\n",
    "Uma rede Encoder-Decoder é composta por duas RNNs: o Encoder e o Decoder.\n",
    "O Encoder lê uma sequência de entrada e gera um vetor único, que o Decoder usa para produzir a sequência de saída.\n",
    "\n",
    "Por simplicidade, utilizaremos a célula GRU. Como sugestão, você também pode explorar outras células RNN, como, por exemplo, a LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "    \"\"\"\n",
    "    Initializes the encoding layer (Encoder) of an RNN.\n",
    "        \n",
    "    Parameters:\n",
    "    - input_size: The number of expected features in the input x\n",
    "    - hidden_size: The number of features in the hidden state h\n",
    "    - dropout_p: If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer\n",
    "    \"\"\"      \n",
    "    super(EncoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    # Embedding layer that transforms word indices into dense vectors of size 'hidden_size'.\n",
    "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "    # Defines the GRU (Gated Recurrent Unit) with both input and output dimensions of 'hidden_size'.\n",
    "    # 'batch_first=True' indicates that the first input dimension is the batch (batch_size, seq_len, hidden_size).\n",
    "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    # Dropout applied after the embedding layer to prevent overfitting.\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, input):\n",
    "    \"\"\"\n",
    "    Defines the forward pass of the model.\n",
    "    \n",
    "    Parameter:\n",
    "    - input: Sequence of word indices of size (batch_size, seq_len).\n",
    "    \n",
    "    Returns:\n",
    "    - output: GRU outputs for each step in the sequence (batch_size, seq_len, hidden_size).\n",
    "    - hidden: The last hidden state vector of the GRU (1, batch_size, hidden_size).\n",
    "        \"\"\"    \n",
    "    embedded = self.dropout(self.embedding(input))\n",
    "    output, hidden = self.gru(embedded)\n",
    "    return output, hidden     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LENGTH defines the sequence length limit\n",
    "MAX_LENGTH = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "  def __init__(self, hidden_size, output_size):\n",
    "    \"\"\"\n",
    "    Initializes the decoding layer (Decoder) of an RNN.\n",
    "    \n",
    "    Parameters:\n",
    "    - hidden_size: The size of the hidden state vector.\n",
    "    - output_size: The size of the output vocabulary (number of unique words in the target language).\n",
    "    \"\"\"    \n",
    "    super(DecoderRNN, self).__init__()\n",
    "\n",
    "    # Embedding layer that transforms word indices into dense vectors of size 'hidden_size'.\n",
    "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "\n",
    "    # Defines a GRU layer that processes input sequences. Both input and output dimensions are 'hidden_size'.\n",
    "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    # Linear layer that maps the hidden state of the GRU to the output vocabulary space.\n",
    "    self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "    \"\"\"\n",
    "    Defines the forward pass for the decoder.\n",
    "    \n",
    "    Parameters:\n",
    "    - encoder_outputs: The output sequence from the encoder (batch_size, seq_len, hidden_size).\n",
    "    - encoder_hidden: The last hidden state of the encoder (1, batch_size, hidden_size).\n",
    "    - target_tensor: The target sequence for teacher forcing (optional).\n",
    "    \n",
    "    Returns:\n",
    "    - decoder_outputs: The output probabilities (log-softmax) for each time step (batch_size, seq_len, output_size).\n",
    "    - decoder_hidden: The final hidden state of the decoder (1, batch_size, hidden_size).\n",
    "    - None: Placeholder for consistency with other methods (e.g., attention).\n",
    "    \"\"\"    \n",
    "    batch_size = encoder_outputs.size(0)\n",
    "\n",
    "    # Initial decoder input is the <SOS> token for every sequence in the batch.\n",
    "    decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_outputs = []\n",
    "\n",
    "    # Loop through each time step\n",
    "    for i in range(MAX_LENGTH):\n",
    "      # Call the forward_step function to get output and hidden state for the current time step.\n",
    "      decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "      decoder_outputs.append(decoder_output)\n",
    "\n",
    "      if target_tensor is not None:\n",
    "        # Teacher forcing: Use the target token as the next input to the decoder.\n",
    "        decoder_input = target_tensor[:, i].unsqueeze(1) \n",
    "      else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        _, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze(-1).detach()  \n",
    "\n",
    "    # Concatenate all the decoder outputs along the time step dimension.\n",
    "    decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "\n",
    "    # Apply log softmax to the outputs to get log-probabilities.\n",
    "    decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "\n",
    "    # Return the outputs, final hidden state, and `None` (for consistency with attention models implemented next).\n",
    "    return decoder_outputs, decoder_hidden, None \n",
    "\n",
    "  def forward_step(self, input, hidden):\n",
    "    \"\"\"\n",
    "    Processes a single step in the decoding sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    - input: The current input to the decoder (batch_size, 1).\n",
    "    - hidden: The current hidden state of the decoder (1, batch_size, hidden_size).\n",
    "    \n",
    "    Returns:\n",
    "    - output: The predicted output (batch_size, 1, output_size).\n",
    "    - hidden: The updated hidden state (1, batch_size, hidden_size).\n",
    "    \"\"\"    \n",
    "    output = self.embedding(input)\n",
    "    output = F.relu(output)\n",
    "    output, hidden = self.gru(output, hidden)\n",
    "    output = self.out(output)\n",
    "    return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Treinando uma época**\n",
    "\n",
    "Para treinar, inserimos a sequência de entrada no Encoder, e capturamos  cada saída e o último hidden state. \n",
    "A partir disso, o Decoder é alimentado com o token de início de sequência (SOS) e o último hidden state do Encoder.\n",
    "\n",
    "Para acelerar a convergência do treinamento, utilizamos o Teacher Forcing, ou seja, realimentamos o Decoder com as palavras esperadas, em vez de usar a última saída gerada pelo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "  \"\"\"\n",
    "  Train the encoder and decoder for one epoch.\n",
    "  \n",
    "  Parameters:\n",
    "  - dataloader: Iterable that provides batches of input and target tensors.\n",
    "  - encoder: The encoder model.\n",
    "  - decoder: The decoder model.\n",
    "  - encoder_optimizer: Optimizer for updating the encoder's parameters.\n",
    "  - decoder_optimizer: Optimizer for updating the decoder's parameters.\n",
    "  - criterion: Loss function to measure the difference between the predicted and target outputs.\n",
    "\n",
    "  Returns:\n",
    "  - The average loss over the epoch.\n",
    "  \"\"\"\n",
    "  total_loss = 0\n",
    "  for data in dataloader:\n",
    "    input_tensor, target_tensor = data\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "    decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "    loss = criterion(\n",
    "        decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "        target_tensor.view(-1)\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001):\n",
    "  loss_points = []\n",
    "  \n",
    "  plot_losses = []\n",
    "  print_loss_total = 0  # Reset every print_every\n",
    "  plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "  encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "  decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "  criterion = nn.NLLLoss()\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    loss_points.append(loss)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      print(f'Epoch: {epoch:4}/{n_epochs:4} - Loss: {loss:.4f}')\n",
    "\n",
    "  \n",
    "  return loss_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 1024\n",
    "\n",
    "train_dataloader = get_dataloader(batch_size, pairs, input_dict, output_dict)\n",
    "\n",
    "encoder = EncoderRNN(\n",
    "  input_size = len(input_dict.keys()),\n",
    "  hidden_size = hidden_size\n",
    ").to(device)\n",
    "\n",
    "decoder = DecoderRNN(\n",
    "  hidden_size = hidden_size,\n",
    "  output_size = len(output_dict.keys())\n",
    ").to(device)\n",
    "\n",
    "\n",
    "loss_points = train(train_dataloader, encoder, decoder, 100, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_loss(loss_points):\n",
    "  plt.plot(loss_points);\n",
    "  plt.title('Training Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "plot_train_loss(loss_points);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Avaliando a predição**\n",
    "\n",
    "Para avaliar a predição, usamos o modelo de forma semelhante ao treinamento, mas sem utilizar o Teacher Forcing. Simplesmente realimentamos o decoder com suas próprias predições a cada passo. Quando o decoder produz o token EOS, o processo é interrompido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence):\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  with torch.no_grad():\n",
    "    input_tensor = tensorFromSentence(input_dict, sentence)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "    decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "    _, topi = decoder_outputs.topk(1)\n",
    "    decoded_ids = topi.squeeze()\n",
    "\n",
    "    decoded_words = []\n",
    "    for idx in decoded_ids:\n",
    "      if idx.item() == EOS_token:\n",
    "        # decoded_words.append('<EOS>')\n",
    "        break\n",
    "      decoded_words.append(output_dict_reverse[idx.item()])\n",
    "  return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=5):\n",
    "  for i in range(n):\n",
    "    pair = random.choice(pairs)\n",
    "    print(f'Input: {pair[0]}')\n",
    "    print(f'Ground truth: {pair[1]}')\n",
    "    output_words, _ = evaluate(encoder, decoder, pair[0])\n",
    "    output_sentence = ''.join(output_words)\n",
    "    print(f'Predicted: {output_sentence}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# **3. Adicionando Atenção**\n",
    "\n",
    "A atenção permite que a rede do decoder \"foque\" em uma parte diferente das saídas do encoder a cada passo das suas próprias saídas. Primeiro, calculamos um conjunto de pesos de atenção. Esses pesos serão multiplicados pelos vetores de saída do encoder para criar uma combinação ponderada. O resultado (chamado attn_applied no código) deve conter informações sobre aquela parte específica da sequência de entrada e, assim, ajudar o decoder a escolher as palavras corretas na saída.\n",
    "\n",
    "![](https://i.imgur.com/1152PYf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bahdanau attention, also known as additive attention, is a commonly used\n",
    "# attention mechanism in sequence-to-sequence models, particularly in\n",
    "# neural machine translation tasks. It was introduced by Bahdanau et al.\n",
    "# in their paper titled [Neural Machine Translation by Jointly Learning to\n",
    "# Align and Translate](https://arxiv.org/pdf/1409.0473.pdf). This\n",
    "# attention mechanism employs a learned alignment model to compute\n",
    "# attention scores between the encoder and decoder hidden states. It\n",
    "# utilizes a feed-forward neural network to calculate alignment scores.\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "  def __init__(self, hidden_size):\n",
    "    '''Initialize the BahdanauAttention class.\n",
    "        \n",
    "    Parameters:\n",
    "    - hidden_size: The size of the hidden state used in the attention mechanism.\n",
    "    '''\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "    self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "    self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "  def forward(self, query, keys):\n",
    "    scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "    scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    context = torch.bmm(weights, keys)\n",
    "    return context, weights  # Return the context vector and attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "  def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "    \"\"\"\n",
    "    Initializes the decoding layer (Decoder with Attention) of an RNN.\n",
    "    \n",
    "    Parameters:\n",
    "    - hidden_size: The size of the hidden state vector.\n",
    "    - output_size: The size of the output vocabulary (number of unique words in the target language).\n",
    "    - dropout_p: The dropout probability for regularization during training.\n",
    "    \"\"\"    \n",
    "    super(AttnDecoderRNN, self).__init__()\n",
    "    \n",
    "    # Embedding layer transforms word indices into dense vectors of size 'hidden_size'.\n",
    "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "    \n",
    "    # Attention mechanism to compute context vectors from encoder outputs.\n",
    "    self.attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "    # GRU layer that processes the combined input of embeddings and context vectors.\n",
    "    self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    # Linear layer maps the GRU output to the output vocabulary space.\n",
    "    self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    # Dropout layer for regularization to prevent overfitting during training.\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "    \"\"\"\n",
    "    Defines the forward pass for the attention decoder.\n",
    "    \n",
    "    Parameters:\n",
    "    - encoder_outputs: The output sequences from the encoder (shape: batch_size, seq_len, hidden_size).\n",
    "    - encoder_hidden: The last hidden state of the encoder (shape: 1, batch_size, hidden_size).\n",
    "    - target_tensor: The target sequence for teacher forcing (optional).\n",
    "    \n",
    "    Returns:\n",
    "    - decoder_outputs: The output probabilities (log-softmax) for each time step (shape: batch_size, seq_len, output_size).\n",
    "    - decoder_hidden: The final hidden state of the decoder (shape: 1, batch_size, hidden_size).\n",
    "    - attentions: The attention weights for each time step (shape: batch_size, seq_len).\n",
    "    \"\"\"        \n",
    "    batch_size = encoder_outputs.size(0)\n",
    "    decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_outputs = []\n",
    "    attentions = []\n",
    "\n",
    "    # Loop through each time step to generate the output sequence.\n",
    "    for i in range(MAX_LENGTH):\n",
    "      # Call the forward_step function to get output, hidden state, and attention weights for the current time step.\n",
    "      decoder_output, decoder_hidden, attn_weights = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
    "      decoder_outputs.append(decoder_output)\n",
    "      attentions.append(attn_weights)\n",
    "\n",
    "      if target_tensor is not None:\n",
    "        # Teacher forcing: Feed the target token as the next input to the decoder.\n",
    "        decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "      else:\n",
    "        # Without teacher forcing: Use its own predictions as the next input.\n",
    "        _, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "    # Concatenate all decoder outputs along the time step dimension.\n",
    "    decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "\n",
    "    # Apply log softmax to the outputs to get log-probabilities.\n",
    "    decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "\n",
    "    # Concatenate all attention weights along the time step dimension.\n",
    "    attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "    # Return the outputs, final hidden state, and attention weights.\n",
    "    return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "  def forward_step(self, input, hidden, encoder_outputs):\n",
    "    \"\"\"\n",
    "    Processes a single step in the decoding sequence with attention.\n",
    "    \n",
    "    Parameters:\n",
    "    - input: The current input to the decoder (shape: batch_size, 1).\n",
    "    - hidden: The current hidden state of the decoder (shape: 1, batch_size, hidden_size).\n",
    "    - encoder_outputs: The outputs from the encoder (shape: batch_size, seq_len, hidden_size).\n",
    "    \n",
    "    Returns:\n",
    "    - output: The predicted output (shape: batch_size, 1, output_size).\n",
    "    - hidden: The updated hidden state (shape: 1, batch_size, hidden_size).\n",
    "    - attn_weights: The attention weights for the current input (shape: batch_size, seq_len).\n",
    "    \"\"\"       \n",
    "    # Get the embedded representation of the current input with dropout for regularization. \n",
    "    embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "    # Permute hidden state to match the query shape for the attention mechanism.\n",
    "    query = hidden.permute(1, 0, 2)\n",
    "\n",
    "    # Compute the context vector and attention weights using the attention mechanism.\n",
    "    context, attn_weights = self.attention(query, encoder_outputs)\n",
    "\n",
    "    # Concatenate the embedded input and the context vector for the GRU.\n",
    "    input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "    # Pass the combined input through the GRU layer to get the output and updated hidden state.\n",
    "    output, hidden = self.gru(input_gru, hidden)\n",
    "\n",
    "    # Map the GRU output to the output vocabulary space using the linear layer.\n",
    "    output = self.out(output)\n",
    "\n",
    "    return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 1024\n",
    "\n",
    "train_dataloader = get_dataloader(batch_size, pairs, input_dict, output_dict)\n",
    "\n",
    "encoder = EncoderRNN(\n",
    "  input_size = len(input_dict.keys()),\n",
    "  hidden_size = hidden_size\n",
    ").to(device)\n",
    "\n",
    "# decoder = DecoderRNN(\n",
    "decoder = AttnDecoderRNN(\n",
    "  hidden_size = hidden_size,\n",
    "  output_size = len(output_dict.keys())\n",
    ").to(device)\n",
    "\n",
    "\n",
    "loss_points = train(train_dataloader, encoder, decoder, 100, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_loss(loss_points);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizando as máscaras de Atenção**\n",
    "\n",
    "Uma propriedade útil do mecanismo de atenção é a alta interpretabilidade de suas saídas. Como ele é usado para atribuir pesos específicos às saídas do encoder da sequência de entrada, podemos imaginar onde a rede está mais focada em cada passo de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot()\n",
    "  data = attentions.cpu().numpy().squeeze()\n",
    "  cax = ax.matshow(data, cmap='bone')\n",
    "  fig.colorbar(cax)\n",
    "\n",
    "  # Set up axes\n",
    "  input_tokens = [token for token in re.split('([^a-zA-Z0-9])', input_sentence) if token.strip()]\n",
    "  ax.set_xticks(ticks=range(len(input_tokens)), labels=input_tokens)\n",
    "  ax.set_yticks(ticks=range(len(output_words)), labels=output_words)\n",
    "\n",
    "  # Show label at every tick\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  for (i, j), z in np.ndenumerate(data):\n",
    "    ax.text(j, i, '{:0.1f}'.format(z), ha='center', va='center')\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "  output_words, attentions = evaluate(encoder, decoder, input_sentence)\n",
    "  print('input =', input_sentence)\n",
    "  print('output =', ''.join(output_words))\n",
    "  showAttention(input_sentence, output_words, attentions[0, :len(output_words), 1:])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "evaluateAndShowAttention('20 de novembro de 2015')\n",
    "\n",
    "evaluateAndShowAttention('22 de maio 1981')\n",
    "\n",
    "evaluateAndShowAttention('15.11.2013')\n",
    "\n",
    "evaluateAndShowAttention('01 de jan. 1989')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#  **4. Exercícios de Exploração Sugeridos**\n",
    "\n",
    "1. **Experimentar com outros datasets:**\n",
    "   - Tente utilizar um dataset diferente para observar como o modelo se comporta com dados novos. Algumas sugestões incluem:\n",
    "     - **Outro par de idiomas**: Experimente traduzir entre diferentes idiomas para testar a flexibilidade do modelo Seq2Seq.\n",
    "     - **Chat → Resposta**: Utilize um dataset de conversas e explore o mapeamento de mensagens para respostas.\n",
    "     - **Pergunta → Resposta**: Experimente um dataset de perguntas e respostas, onde o modelo aprende a fornecer respostas a perguntas específicas.\n",
    "\n",
    "2. **Explorar codificações alternativas:**\n",
    "   - Em vez de utilizar representações simples de palavras, tente usar **embeddings de palavras**. Isso pode melhorar a capacidade do modelo de capturar relacionamentos semânticos entre as palavras. Exemplos incluem: **Word2Vec** ou **GloVe**: Substitua as embeddings atuais por embeddings pré-treinadas para aproveitar o conhecimento adicional aprendido em grandes corpora.\n",
    "   - Experimente outras maneiras de **tokenizar o dataset**. Por exemplo, você pode usar subpalavras (como BPE - Byte Pair Encoding) ou tokens baseados em caracteres em vez de palavras inteiras.\n",
    "\n",
    "3. **Ajustes na arquitetura do modelo:**\n",
    "   - **Aumentar a complexidade do modelo**: Tente aumentar o número de camadas, o número de unidades ocultas (*hidden units*) e experimente treinar com sentenças mais longas. Compare o tempo de treinamento e a qualidade dos resultados obtidos. Isso pode revelar como o modelo escala com a complexidade.\n",
    "   - Em vez de utilizar células GRU, tente implementar o modelo com células **LSTM**. Compare os resultados e observe as diferenças no tempo de convergência e na qualidade da tradução.\n",
    "\n",
    "   - **Treinamento de um autoencoder**:\n",
    "     - Use um arquivo de tradução em que as frases de entrada e saída sejam idênticas (exemplo: \"Eu sou um teste \\t Eu sou um teste\").\n",
    "     - Treine o modelo como um **autoencoder**.\n",
    "     - Após o treinamento, **salve apenas a rede Encoder** e, em seguida, treine um novo Decoder para realizar a tradução a partir das representações do Encoder.\n",
    "\n",
    "4. **Explorar diferentes mecanismos de atenção:**\n",
    "   - Tente usar outro tipo de **mecanismo de atenção** (como **multi-head attention**) e compare os resultados em termos de precisão e capacidade de generalização.\n",
    "\n",
    "5. **Revisão e Contribuição:**\n",
    "   - Como o notebook não foi extensivamente testado, sinta-se à vontade para procurar e corrigir quaisquer erros que encontrar. Agradecemos qualquer contribuição para melhorar o código e a documentação. Seu feedback é valioso para aprimorar a experiência de aprendizado para todos os alunos.\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
