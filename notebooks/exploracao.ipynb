{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=cuda\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# SET UP\n",
    "#################################\n",
    "\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device={device}')\n",
    "\n",
    "#################################\n",
    "# CONSTANTES\n",
    "#################################\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções e Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# FUNÇÕES\n",
    "#################################\n",
    "\n",
    "def readDataset():\n",
    "  file = DATA_DIR + 'datahumana-datamaquina.txt'\n",
    "  # file = '' # Arquivo a ser lido na formatação entrada<tab>saida\n",
    "  lines = open(file, encoding='utf-8').read().strip().split('\\n')\n",
    "  pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "  print(f'Read {len(pairs)} samples from {file}')\n",
    "  print()\n",
    "  print(\"\\n\".join(map(str, random.sample(pairs, 15))))\n",
    "  return pairs\n",
    "  \n",
    "def prepareData(pairs):\n",
    "  input_dict  = {\"SOS\": SOS_token, \"EOS\": EOS_token}\n",
    "  output_dict = {\"SOS\": SOS_token, \"EOS\": EOS_token}\n",
    "\n",
    "  input_dict_reverse  = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "  output_dict_reverse = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "\n",
    "  for pair in pairs:\n",
    "    ipt, tgt = pair\n",
    "    \n",
    "    mask = '([^a-zA-Z0-9])'\n",
    "    result = [token for token in re.split(mask, ipt) if token.strip()]\n",
    "    for word_input in result:\n",
    "      if word_input not in input_dict:\n",
    "        next_int = max(input_dict.values()) + 1\n",
    "        input_dict[word_input] = next_int\n",
    "        input_dict_reverse[next_int] = word_input\n",
    "\n",
    "    for word_output in re.split(mask, tgt):\n",
    "      if word_output not in output_dict:\n",
    "        next_int = max(output_dict.values()) + 1\n",
    "        output_dict[word_output] = next_int\n",
    "        output_dict_reverse[next_int] = word_output\n",
    "\n",
    "  print(f'Number of input words: {len(input_dict.keys())}')\n",
    "  print(f'Number of output words: {len(output_dict.keys())}')\n",
    "\n",
    "  return input_dict, output_dict, input_dict_reverse, output_dict_reverse\n",
    "\n",
    "def indexesFromSentence(language_dict, sentence, verbose=False):\n",
    "  '''\n",
    "    Função utilitária para converter uma sentença em índices\n",
    "  '''\n",
    "  encoded = [language_dict[word] for word in re.split('([^a-zA-Z0-9])', sentence) if word.strip()]\n",
    "  if verbose:\n",
    "    print(f\"'{sentence}' => {encoded}\")\n",
    "  return encoded\n",
    "  \n",
    "# Funções utilitárias\n",
    "def tensorFromSentence(language_dict, sentence):\n",
    "  indexes = indexesFromSentence(language_dict, sentence)\n",
    "  indexes.append(EOS_token)\n",
    "  return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_dict, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_dict, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "  \n",
    "def get_dataloader(batch_size, pairs, input_dict, output_dict):\n",
    "  n = len(pairs)\n",
    "  input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "  target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "  for idx, (inp, tgt) in enumerate(pairs):\n",
    "    inp_ids = indexesFromSentence(input_dict, inp)\n",
    "    tgt_ids = indexesFromSentence(output_dict, tgt)\n",
    "    inp_ids.append(EOS_token)\n",
    "    tgt_ids.append(EOS_token)\n",
    "    input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "    target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "  train_data = TensorDataset(torch.LongTensor(input_ids).to(device), torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "  train_sampler = RandomSampler(train_data)\n",
    "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "  return train_dataloader\n",
    "  \n",
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "  \"\"\"\n",
    "  Train the encoder and decoder for one epoch.\n",
    "  \n",
    "  Parameters:\n",
    "  - dataloader: Iterable that provides batches of input and target tensors.\n",
    "  - encoder: The encoder model.\n",
    "  - decoder: The decoder model.\n",
    "  - encoder_optimizer: Optimizer for updating the encoder's parameters.\n",
    "  - decoder_optimizer: Optimizer for updating the decoder's parameters.\n",
    "  - criterion: Loss function to measure the difference between the predicted and target outputs.\n",
    "\n",
    "  Returns:\n",
    "  - The average loss over the epoch.\n",
    "  \"\"\"\n",
    "  total_loss = 0\n",
    "  for data in dataloader:\n",
    "    input_tensor, target_tensor = data\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "    decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "    loss = criterion(\n",
    "        decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "        target_tensor.view(-1)\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  return total_loss / len(dataloader)\n",
    "  \n",
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001):\n",
    "  loss_points = []\n",
    "  \n",
    "  plot_losses = []\n",
    "  print_loss_total = 0  # Reset every print_every\n",
    "  plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "  encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "  decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "  criterion = nn.NLLLoss()\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    loss_points.append(loss)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      print(f'Epoch: {epoch:4}/{n_epochs:4} - Loss: {loss:.4f}')\n",
    "\n",
    "  \n",
    "  return loss_points\n",
    "\n",
    "def plot_train_loss(loss_points):\n",
    "  plt.plot(loss_points);\n",
    "  plt.title('Training Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  \n",
    "def evaluate(encoder, decoder, sentence):\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  with torch.no_grad():\n",
    "    input_tensor = tensorFromSentence(input_dict, sentence)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "    decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "    _, topi = decoder_outputs.topk(1)\n",
    "    decoded_ids = topi.squeeze()\n",
    "\n",
    "    decoded_words = []\n",
    "    for idx in decoded_ids:\n",
    "      if idx.item() == EOS_token:\n",
    "        # decoded_words.append('<EOS>')\n",
    "        break\n",
    "      decoded_words.append(output_dict_reverse[idx.item()])\n",
    "  return decoded_words, decoder_attn\n",
    "  \n",
    "def evaluateRandomly(encoder, decoder, n=5):\n",
    "  for i in range(n):\n",
    "    pair = random.choice(pairs)\n",
    "    print(f'Input: {pair[0]}')\n",
    "    print(f'Ground truth: {pair[1]}')\n",
    "    output_words, _ = evaluate(encoder, decoder, pair[0])\n",
    "    output_sentence = ''.join(output_words)\n",
    "    print(f'Predicted: {output_sentence}')\n",
    "    print('')\n",
    "\n",
    "  def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "    \"\"\"\n",
    "    Defines the forward pass for the attention decoder.\n",
    "    \n",
    "    Parameters:\n",
    "    - encoder_outputs: The output sequences from the encoder (shape: batch_size, seq_len, hidden_size).\n",
    "    - encoder_hidden: The last hidden state of the encoder (shape: 1, batch_size, hidden_size).\n",
    "    - target_tensor: The target sequence for teacher forcing (optional).\n",
    "    \n",
    "    Returns:\n",
    "    - decoder_outputs: The output probabilities (log-softmax) for each time step (shape: batch_size, seq_len, output_size).\n",
    "    - decoder_hidden: The final hidden state of the decoder (shape: 1, batch_size, hidden_size).\n",
    "    - attentions: The attention weights for each time step (shape: batch_size, seq_len).\n",
    "    \"\"\"        \n",
    "    batch_size = encoder_outputs.size(0)\n",
    "    decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_outputs = []\n",
    "    attentions = []\n",
    "\n",
    "    # Loop through each time step to generate the output sequence.\n",
    "    for i in range(MAX_LENGTH):\n",
    "      # Call the forward_step function to get output, hidden state, and attention weights for the current time step.\n",
    "      decoder_output, decoder_hidden, attn_weights = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
    "      decoder_outputs.append(decoder_output)\n",
    "      attentions.append(attn_weights)\n",
    "\n",
    "      if target_tensor is not None:\n",
    "        # Teacher forcing: Feed the target token as the next input to the decoder.\n",
    "        decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "      else:\n",
    "        # Without teacher forcing: Use its own predictions as the next input.\n",
    "        _, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "    # Concatenate all decoder outputs along the time step dimension.\n",
    "    decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "\n",
    "    # Apply log softmax to the outputs to get log-probabilities.\n",
    "    decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "\n",
    "    # Concatenate all attention weights along the time step dimension.\n",
    "    attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "    # Return the outputs, final hidden state, and attention weights.\n",
    "    return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "  def forward_step(self, input, hidden, encoder_outputs):\n",
    "    \"\"\"\n",
    "    Processes a single step in the decoding sequence with attention.\n",
    "    \n",
    "    Parameters:\n",
    "    - input: The current input to the decoder (shape: batch_size, 1).\n",
    "    - hidden: The current hidden state of the decoder (shape: 1, batch_size, hidden_size).\n",
    "    - encoder_outputs: The outputs from the encoder (shape: batch_size, seq_len, hidden_size).\n",
    "    \n",
    "    Returns:\n",
    "    - output: The predicted output (shape: batch_size, 1, output_size).\n",
    "    - hidden: The updated hidden state (shape: 1, batch_size, hidden_size).\n",
    "    - attn_weights: The attention weights for the current input (shape: batch_size, seq_len).\n",
    "    \"\"\"       \n",
    "    # Get the embedded representation of the current input with dropout for regularization. \n",
    "    embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "    # Permute hidden state to match the query shape for the attention mechanism.\n",
    "    query = hidden.permute(1, 0, 2)\n",
    "\n",
    "    # Compute the context vector and attention weights using the attention mechanism.\n",
    "    context, attn_weights = self.attention(query, encoder_outputs)\n",
    "\n",
    "    # Concatenate the embedded input and the context vector for the GRU.\n",
    "    input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "    # Pass the combined input through the GRU layer to get the output and updated hidden state.\n",
    "    output, hidden = self.gru(input_gru, hidden)\n",
    "\n",
    "    # Map the GRU output to the output vocabulary space using the linear layer.\n",
    "    output = self.out(output)\n",
    "\n",
    "    return output, hidden, attn_weights\n",
    "\n",
    "  \n",
    "#################################\n",
    "# CLASSES\n",
    "#################################\n",
    "  \n",
    "class EncoderRNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "    \"\"\"\n",
    "    Initializes the encoding layer (Encoder) of an RNN.\n",
    "        \n",
    "    Parameters:\n",
    "    - input_size: The number of expected features in the input x\n",
    "    - hidden_size: The number of features in the hidden state h\n",
    "    - dropout_p: If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer\n",
    "    \"\"\"      \n",
    "    super(EncoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    # Embedding layer that transforms word indices into dense vectors of size 'hidden_size'.\n",
    "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "    # Defines the GRU (Gated Recurrent Unit) with both input and output dimensions of 'hidden_size'.\n",
    "    # 'batch_first=True' indicates that the first input dimension is the batch (batch_size, seq_len, hidden_size).\n",
    "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    # Dropout applied after the embedding layer to prevent overfitting.\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, input):\n",
    "    \"\"\"\n",
    "    Defines the forward pass of the model.\n",
    "    \n",
    "    Parameter:\n",
    "    - input: Sequence of word indices of size (batch_size, seq_len).\n",
    "    \n",
    "    Returns:\n",
    "    - output: GRU outputs for each step in the sequence (batch_size, seq_len, hidden_size).\n",
    "    - hidden: The last hidden state vector of the GRU (1, batch_size, hidden_size).\n",
    "        \"\"\"    \n",
    "    embedded = self.dropout(self.embedding(input))\n",
    "    output, hidden = self.gru(embedded)\n",
    "    return output, hidden \n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "  def __init__(self, hidden_size, output_size):\n",
    "    \"\"\"\n",
    "    Initializes the decoding layer (Decoder) of an RNN.\n",
    "    \n",
    "    Parameters:\n",
    "    - hidden_size: The size of the hidden state vector.\n",
    "    - output_size: The size of the output vocabulary (number of unique words in the target language).\n",
    "    \"\"\"    \n",
    "    super(DecoderRNN, self).__init__()\n",
    "\n",
    "    # Embedding layer that transforms word indices into dense vectors of size 'hidden_size'.\n",
    "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "\n",
    "    # Defines a GRU layer that processes input sequences. Both input and output dimensions are 'hidden_size'.\n",
    "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    # Linear layer that maps the hidden state of the GRU to the output vocabulary space.\n",
    "    self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "    \"\"\"\n",
    "    Defines the forward pass for the decoder.\n",
    "    \n",
    "    Parameters:\n",
    "    - encoder_outputs: The output sequence from the encoder (batch_size, seq_len, hidden_size).\n",
    "    - encoder_hidden: The last hidden state of the encoder (1, batch_size, hidden_size).\n",
    "    - target_tensor: The target sequence for teacher forcing (optional).\n",
    "    \n",
    "    Returns:\n",
    "    - decoder_outputs: The output probabilities (log-softmax) for each time step (batch_size, seq_len, output_size).\n",
    "    - decoder_hidden: The final hidden state of the decoder (1, batch_size, hidden_size).\n",
    "    - None: Placeholder for consistency with other methods (e.g., attention).\n",
    "    \"\"\"    \n",
    "    batch_size = encoder_outputs.size(0)\n",
    "\n",
    "    # Initial decoder input is the <SOS> token for every sequence in the batch.\n",
    "    decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_outputs = []\n",
    "\n",
    "    # Loop through each time step\n",
    "    for i in range(MAX_LENGTH):\n",
    "      # Call the forward_step function to get output and hidden state for the current time step.\n",
    "      decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "      decoder_outputs.append(decoder_output)\n",
    "\n",
    "      if target_tensor is not None:\n",
    "        # Teacher forcing: Use the target token as the next input to the decoder.\n",
    "        decoder_input = target_tensor[:, i].unsqueeze(1) \n",
    "      else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        _, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze(-1).detach()  \n",
    "\n",
    "    # Concatenate all the decoder outputs along the time step dimension.\n",
    "    decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "\n",
    "    # Apply log softmax to the outputs to get log-probabilities.\n",
    "    decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "\n",
    "    # Return the outputs, final hidden state, and `None` (for consistency with attention models implemented next).\n",
    "    return decoder_outputs, decoder_hidden, None \n",
    "\n",
    "  def forward_step(self, input, hidden):\n",
    "    \"\"\"\n",
    "    Processes a single step in the decoding sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    - input: The current input to the decoder (batch_size, 1).\n",
    "    - hidden: The current hidden state of the decoder (1, batch_size, hidden_size).\n",
    "    \n",
    "    Returns:\n",
    "    - output: The predicted output (batch_size, 1, output_size).\n",
    "    - hidden: The updated hidden state (1, batch_size, hidden_size).\n",
    "    \"\"\"    \n",
    "    output = self.embedding(input)\n",
    "    output = F.relu(output)\n",
    "    output, hidden = self.gru(output, hidden)\n",
    "    output = self.out(output)\n",
    "    return output, hidden    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Bahdanau attention, also known as additive attention, is a commonly used\n",
    "# attention mechanism in sequence-to-sequence models, particularly in\n",
    "# neural machine translation tasks. It was introduced by Bahdanau et al.\n",
    "# in their paper titled [Neural Machine Translation by Jointly Learning to\n",
    "# Align and Translate](https://arxiv.org/pdf/1409.0473.pdf). This\n",
    "# attention mechanism employs a learned alignment model to compute\n",
    "# attention scores between the encoder and decoder hidden states. It\n",
    "# utilizes a feed-forward neural network to calculate alignment scores.\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "  def __init__(self, hidden_size):\n",
    "    '''Initialize the BahdanauAttention class.\n",
    "        \n",
    "    Parameters:\n",
    "    - hidden_size: The size of the hidden state used in the attention mechanism.\n",
    "    '''\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "    self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "    self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "  def forward(self, query, keys):\n",
    "    scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "    scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    context = torch.bmm(weights, keys)\n",
    "    return context, weights  # Return the context vector and attention weights.\n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "  def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "    \"\"\"\n",
    "    Initializes the decoding layer (Decoder with Attention) of an RNN.\n",
    "    \n",
    "    Parameters:\n",
    "    - hidden_size: The size of the hidden state vector.\n",
    "    - output_size: The size of the output vocabulary (number of unique words in the target language).\n",
    "    - dropout_p: The dropout probability for regularization during training.\n",
    "    \"\"\"    \n",
    "    super(AttnDecoderRNN, self).__init__()\n",
    "    \n",
    "    # Embedding layer transforms word indices into dense vectors of size 'hidden_size'.\n",
    "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "    \n",
    "    # Attention mechanism to compute context vectors from encoder outputs.\n",
    "    self.attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "    # GRU layer that processes the combined input of embeddings and context vectors.\n",
    "    self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    # Linear layer maps the GRU output to the output vocabulary space.\n",
    "    self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    # Dropout layer for regularization to prevent overfitting during training.\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq data -> data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem camada de atenção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# CONFIGURAÇÃO DO EXPERIMENTO\n",
    "#################################\n",
    "# MAX_LENGTH defines the sequence length limit\n",
    "MAX_LENGTH = 18\n",
    "\n",
    "hidden_size = 128\n",
    "batch_size = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando a leitura do dataset:\n",
      "Read 10000 samples from ../data/datahumana-datamaquina.txt\n",
      "\n",
      "['domingo 8 de fevereiro de 1998', '08/02/1998']\n",
      "['26.01.85', '26/01/1985']\n",
      "['domingo 5 de outubro de 1980', '05/10/1980']\n",
      "['16 de julho de 1998', '16/07/1998']\n",
      "['11 outubro 2014', '11/10/2014']\n",
      "['14 jul. 2014', '14/07/2014']\n",
      "['18 janeiro 1977', '18/01/1977']\n",
      "['agosto 28 1978', '28/08/1978']\n",
      "['abril 21 2015', '21/04/2015']\n",
      "['terca-feira 29 de novembro de 1988', '29/11/1988']\n",
      "['02/01/2018', '02/01/2018']\n",
      "['10/06/1993', '10/06/1993']\n",
      "['3 marco 1978', '03/03/1978']\n",
      "['26 de setembro de 2012', '26/09/2012']\n",
      "['8 de jul. de 2018', '08/07/2018']\n",
      "\n",
      "\n",
      "Realizando a preparacao dos dados:\n",
      "Number of input words: 164\n",
      "Number of output words: 89\n",
      "'segunda-feira 18 de outubro de 1999' => [33, 11, 12, 102, 7, 66, 7, 39]\n",
      "'30/09/2022' => [46, 3, 7, 3, 47]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m indexesFromSentence(input_dict, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegunda-feira 18 de outubro de 1999\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m);\n\u001b[1;32m     15\u001b[0m indexesFromSentence(output_dict, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m30/09/2022\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m);\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtensorsFromPair\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msegunda-feira 18 de outubro de 1999\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m18/10/1999\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 60\u001b[0m, in \u001b[0;36mtensorsFromPair\u001b[0;34m(pair)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensorsFromPair\u001b[39m(pair):\n\u001b[0;32m---> 60\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensorFromSentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     target_tensor \u001b[38;5;241m=\u001b[39m tensorFromSentence(output_dict, pair[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (input_tensor, target_tensor)\n",
      "Cell \u001b[0;32mIn[3], line 57\u001b[0m, in \u001b[0;36mtensorFromSentence\u001b[0;34m(language_dict, sentence)\u001b[0m\n\u001b[1;32m     55\u001b[0m indexes \u001b[38;5;241m=\u001b[39m indexesFromSentence(language_dict, sentence)\n\u001b[1;32m     56\u001b[0m indexes\u001b[38;5;241m.\u001b[39mappend(EOS_token)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# PREPARAÇÂO DOS DADOS\n",
    "#################################\n",
    "\n",
    "# leitura do dataset\n",
    "print('Realizando a leitura do dataset:')\n",
    "pairs = readDataset()\n",
    "\n",
    "# preparação do data\n",
    "print('\\n\\nRealizando a preparacao dos dados:')\n",
    "input_dict, output_dict, input_dict_reverse, output_dict_reverse = prepareData(pairs)\n",
    "\n",
    "\n",
    "indexesFromSentence(input_dict, 'segunda-feira 18 de outubro de 1999', verbose=True);\n",
    "indexesFromSentence(output_dict, '30/09/2022', verbose=True);\n",
    "tensorsFromPair(['segunda-feira 18 de outubro de 1999', '18/10/1999'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=cuda\n",
      "Read 10000 samples from ../data/datahumana-datamaquina.txt\n",
      "\n",
      "['18 ago. 1994', '18/08/1994']\n",
      "['19/10/2006', '19/10/2006']\n",
      "['quarta-feira 29 de setembro de 2010', '29/09/2010']\n",
      "['28 julho 2020', '28/07/2020']\n",
      "['05 mar. 2004', '05/03/2004']\n",
      "['30/04/1991', '30/04/1991']\n",
      "['domingo 24 de outubro de 2021', '24/10/2021']\n",
      "['2 nov. 1982', '02/11/1982']\n",
      "['domingo 19 de agosto de 1984', '19/08/1984']\n",
      "['sexta-feira 19 de abril de 1985', '19/04/1985']\n",
      "['1 de setembro de 2019', '01/09/2019']\n",
      "['sabado 8 de agosto de 1992', '08/08/1992']\n",
      "['quinta-feira 16 de agosto de 2001', '16/08/2001']\n",
      "['quarta-feira 25 de fevereiro de 1981', '25/02/1981']\n",
      "['21 de out. de 2003', '21/10/2003']\n",
      "Number of input words: 164\n",
      "Number of output words: 89\n",
      "'segunda-feira 18 de outubro de 1999' => [33, 11, 12, 102, 7, 66, 7, 39]\n",
      "'30/09/2022' => [46, 3, 7, 3, 47]\n",
      "Epoch:   10/ 100 - Loss: 0.6112\n",
      "Epoch:   20/ 100 - Loss: 0.5300\n",
      "Epoch:   30/ 100 - Loss: 0.3955\n",
      "Epoch:   40/ 100 - Loss: 0.2605\n",
      "Epoch:   50/ 100 - Loss: 0.1746\n",
      "Epoch:   60/ 100 - Loss: 0.0651\n",
      "Epoch:   70/ 100 - Loss: 0.0250\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = get_dataloader(batch_size, pairs, input_dict, output_dict)\n",
    "\n",
    "encoder = EncoderRNN(\n",
    "  input_size = len(input_dict.keys()),\n",
    "  hidden_size = hidden_size\n",
    ").to(device)\n",
    "\n",
    "decoder = DecoderRNN(\n",
    "  hidden_size = hidden_size,\n",
    "  output_size = len(output_dict.keys())\n",
    ").to(device)\n",
    "\n",
    "\n",
    "loss_points = train(train_dataloader, encoder, decoder, 100, learning_rate=0.001)\n",
    "plot_train_loss(loss_points);\n",
    "evaluateRandomly(encoder, decoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com camada de Atenção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# PROCESSO COM ATENÇÃO\n",
    "#################################\n",
    "\n",
    "hidden_size = 128\n",
    "batch_size = 1024\n",
    "\n",
    "train_dataloader = get_dataloader(batch_size, pairs, input_dict, output_dict)\n",
    "\n",
    "encoder = EncoderRNN(\n",
    "  input_size = len(input_dict.keys()),\n",
    "  hidden_size = hidden_size\n",
    ").to(device)\n",
    "\n",
    "# decoder = DecoderRNN(\n",
    "decoder = AttnDecoderRNN(\n",
    "  hidden_size = hidden_size,\n",
    "  output_size = len(output_dict.keys())\n",
    ").to(device)\n",
    "\n",
    "\n",
    "loss_points = train(train_dataloader, encoder, decoder, 100, learning_rate=0.001)\n",
    "plot_train_loss(loss_points)\n",
    "evaluateRandomly(encoder, decoder)\n",
    "\n",
    "%matplotlib inline\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot()\n",
    "  data = attentions.cpu().numpy().squeeze()\n",
    "  cax = ax.matshow(data, cmap='bone')\n",
    "  fig.colorbar(cax)\n",
    "\n",
    "  # Set up axes\n",
    "  input_tokens = [token for token in re.split('([^a-zA-Z0-9])', input_sentence) if token.strip()]\n",
    "  ax.set_xticks(ticks=range(len(input_tokens)), labels=input_tokens)\n",
    "  ax.set_yticks(ticks=range(len(output_words)), labels=output_words)\n",
    "\n",
    "  # Show label at every tick\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  for (i, j), z in np.ndenumerate(data):\n",
    "    ax.text(j, i, '{:0.1f}'.format(z), ha='center', va='center')\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "  output_words, attentions = evaluate(encoder, decoder, input_sentence)\n",
    "  print('input =', input_sentence)\n",
    "  print('output =', ''.join(output_words))\n",
    "  showAttention(input_sentence, output_words, attentions[0, :len(output_words), 1:])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "evaluateAndShowAttention('20 de novembro de 2015')\n",
    "\n",
    "evaluateAndShowAttention('22 de maio 1981')\n",
    "\n",
    "evaluateAndShowAttention('15.11.2013')\n",
    "\n",
    "evaluateAndShowAttention('01 de jan. 1989')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
